name: Provision EKS Cluster with ALB Controller

on:
  push:
    branches:
      - setup
    paths:
      - 'terraform/**'
      - '.github/workflows/provision-eks.yml'
  workflow_dispatch:
    inputs:
      cluster_name:
        description: 'EKS Cluster Name'
        required: false
        default: 'glide-api-cluster'
      region:
        description: 'AWS Region'
        required: false
        default: 'us-east-1'
      setup_alb:
        description: 'Set up ALB Controller'
        required: false
        default: true
        type: boolean

jobs:
  setup-eks:
    runs-on: ubuntu-latest
    if: "contains(github.event.head_commit.message, '[provision-eks]') || github.event_name == 'workflow_dispatch'"
    outputs:
      cluster_created: ${{ steps.cluster-validation.outputs.cluster_created }}
      cluster_name: ${{ env.CLUSTER_NAME }}
      region: ${{ env.AWS_REGION }}

    env:
      CLUSTER_NAME: ${{ github.event.inputs.cluster_name || 'glide-api-cluster' }}
      AWS_REGION: ${{ github.event.inputs.region || 'us-east-1' }}
      TF_VAR_cluster_name: ${{ github.event.inputs.cluster_name || 'glide-api-cluster' }}
      TF_VAR_region: ${{ github.event.inputs.region || 'us-east-1' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.7"

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Terraform Validate
        working-directory: ./terraform
        run: terraform validate

      - name: Terraform Plan
        working-directory: ./terraform
        run: terraform plan -var="cluster_name=${CLUSTER_NAME}" -var="region=${AWS_REGION}" -out=tfplan

      - name: Terraform Apply
        working-directory: ./terraform
        run: terraform apply -auto-approve tfplan

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}"
          echo "Checking for nodes in the cluster..."
          kubectl get nodes -o wide || echo "No nodes found yet, but continuing..."

      - name: Deploy Metrics Server
        run: |
          echo "Deploying Metrics Server..."
          kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

      - name: Wait for nodes to be ready
        run: |
          echo "Waiting for nodes to appear and become ready..."
          
          # Function to check if nodes are ready
          check_nodes_ready() {
            NODES=$(kubectl get nodes -o json | jq -r '.items | length')
            READY_NODES=$(kubectl get nodes -o json | jq -r '.items[] | select(.status.conditions[] | select(.type=="Ready" and .status=="True")) | .metadata.name' | wc -l)
            
            if [ "$NODES" -gt 0 ] && [ "$NODES" -eq "$READY_NODES" ]; then
              return 0
            else
              return 1
            fi
          }
          
          # Try for up to 5 minutes
          for i in {1..30}; do
            if check_nodes_ready; then
              echo "All nodes are ready!"
              break
            fi
            
            echo "Waiting for nodes to become ready (attempt $i/30)..."
            if [ $i -eq 30 ]; then
              echo "Warning: Not all nodes are ready after timeout. Continuing anyway..."
            fi
            
            sleep 10
          done

      - name: Cluster Validation
        id: cluster-validation
        run: |
          echo "Validating EKS cluster..."
          
          if aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" &> /dev/null; then
            echo "EKS control plane is available."
            
            # Check if nodes exist
            NODE_COUNT=$(kubectl get nodes --no-headers 2>/dev/null | wc -l || echo "0")
            
            if [ "$NODE_COUNT" -gt 0 ]; then
              echo "cluster_created=true" >> $GITHUB_OUTPUT
              echo "EKS cluster '${CLUSTER_NAME}' successfully created with $NODE_COUNT nodes."
            else
              echo "cluster_created=false" >> $GITHUB_OUTPUT
              echo "EKS control plane exists but no nodes are running."
            fi
          else
            echo "cluster_created=false" >> $GITHUB_OUTPUT
            echo "EKS cluster creation failed."
          fi

      - name: Cluster Info
        run: |
          echo "EKS Cluster '${CLUSTER_NAME}' deployed in region '${AWS_REGION}'"
          echo "Cluster nodes:"
          kubectl get nodes
          echo "Cluster pods:"
          kubectl get pods --all-namespaces

      - name: Create Status File
        run: |
          mkdir -p cluster-status
          echo "ACTIVE" > cluster-status/status.txt
          echo "${CLUSTER_NAME}" > cluster-status/name.txt
          echo "${AWS_REGION}" > cluster-status/region.txt
          echo "$(date +%s)" > cluster-status/timestamp.txt

      - name: Commit Status
        run: |
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'whyaneel@gmail.com'
          git add cluster-status/
          git commit -m "Update cluster status to ACTIVE [skip ci]" || echo "No changes to commit"
          git push || echo "Failed to push changes"

  setup-alb-controller:
    needs: setup-eks
    if: needs.setup-eks.outputs.cluster_created == 'true' && (github.event.inputs.setup_alb != 'false')
    runs-on: ubuntu-latest
    
    env:
      CLUSTER_NAME: ${{ needs.setup-eks.outputs.cluster_name }}
      AWS_REGION: ${{ needs.setup-eks.outputs.region }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: true

      - name: Verify EKS cluster exists
        id: verify-cluster
        run: |
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} &> /dev/null; then
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
            echo "EKS cluster '${{ env.CLUSTER_NAME }}' exists."
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
            echo "EKS cluster '${{ env.CLUSTER_NAME }}' does not exist. Please create it first."
            exit 1
          fi

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Update kube config
        run: aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Create IAM Policy for ALB Controller
        id: create-iam-policy
        run: |
          echo "Downloading ALB Controller IAM Policy..."
          curl -o alb-controller-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
          
          # Check if policy already exists
          if aws iam get-policy --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/AWSLoadBalancerControllerIAMPolicy &> /dev/null; then
            echo "Policy already exists, skipping creation..."
            POLICY_ARN="arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/AWSLoadBalancerControllerIAMPolicy"
          else
            echo "Creating IAM policy for ALB Controller..."
            POLICY_ARN=$(aws iam create-policy \
              --policy-name AWSLoadBalancerControllerIAMPolicy \
              --policy-document file://alb-controller-policy.json \
              --query 'Policy.Arn' --output text)
          fi
          
          echo "policy_arn=${POLICY_ARN}" >> $GITHUB_OUTPUT
          echo "IAM Policy ARN: ${POLICY_ARN}"

      - name: Install eksctl
        run: |
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version

      - name: Create IAM Service Account for ALB Controller
        run: |
          echo "Creating IAM service account for ALB Controller..."
          # Check if service account already exists
          if kubectl get sa -n kube-system aws-load-balancer-controller &> /dev/null; then
            echo "Service account already exists, updating..."
            # Delete old service account to update it
            kubectl delete sa -n kube-system aws-load-balancer-controller || true
          fi
          
          eksctl create iamserviceaccount \
            --cluster=${{ env.CLUSTER_NAME }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=${{ steps.create-iam-policy.outputs.policy_arn }} \
            --override-existing-serviceaccounts \
            --approve

      - name: Add Helm repo for ALB Controller
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update

      - name: Check if ALB Controller is already installed
        id: check-alb
        run: |
          if helm list -n kube-system | grep aws-load-balancer-controller; then
            echo "alb_installed=true" >> $GITHUB_OUTPUT
          else
            echo "alb_installed=false" >> $GITHUB_OUTPUT
          fi

      - name: Uninstall previous ALB Controller if exists
        if: steps.check-alb.outputs.alb_installed == 'true'
        run: |
          echo "Uninstalling existing ALB Controller..."
          helm uninstall aws-load-balancer-controller -n kube-system

      - name: Install ALB Controller
        run: |
          echo "Installing AWS Load Balancer Controller..."
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ env.CLUSTER_NAME }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ env.AWS_REGION }} \
            --set vpcId=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.resourcesVpcConfig.vpcId" --output text)

      - name: Verify ALB Controller installation
        run: |
          echo "Waiting for AWS Load Balancer Controller deployment to be ready..."
          kubectl -n kube-system wait --for=condition=available --timeout=120s deployment/aws-load-balancer-controller
          
          echo "Checking AWS Load Balancer Controller pods..."
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
          
          echo "Checking AWS Load Balancer Controller deployment..."
          kubectl describe deployment -n kube-system aws-load-balancer-controller

      - name: Create Test Ingress with ALB Annotations
        run: |
          echo "Creating test ingress to verify ALB Controller..."
          cat > test-ingress.yaml << EOF
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: test-alb
            namespace: default
            annotations:
              kubernetes.io/ingress.class: alb
              alb.ingress.kubernetes.io/scheme: internet-facing
              alb.ingress.kubernetes.io/target-type: ip
          spec:
            rules:
            - http:
                paths:
                - path: /test
                  pathType: Prefix
                  backend:
                    service:
                      name: kubernetes
                      port:
                        number: 443
          EOF
          
          kubectl apply -f test-ingress.yaml
          
          echo "Waiting for ALB to be provisioned (this may take a few minutes)..."
          attempts=0
          while [ $attempts -lt 10 ]; do
            attempts=$((attempts+1))
            if kubectl get ingress test-alb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' | grep -q "amazonaws.com"; then
              echo "ALB successfully provisioned!"
              echo "ALB URL: $(kubectl get ingress test-alb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')"
              break
            fi
            echo "Waiting for ALB... (attempt $attempts/10)"
            sleep 30
          done
          
          # Clean up test ingress
          kubectl delete ingress test-alb

      - name: Final status update
        run: |
          echo "âœ… EKS Cluster with ALB Controller setup complete!"
          echo "Cluster name: ${{ env.CLUSTER_NAME }}"
          echo "Region: ${{ env.AWS_REGION }}"
          echo "ALB Controller status:"
          kubectl get deployment -n kube-system aws-load-balancer-controller
          
          # Apr 09 2025 03
